#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
C86 Client360 Product Appropriateness â€” SAS -> Python one-to-one replica
Driver: pandas + teradatasql (no volatile tables)
Outputs: detail + autocomplete Excel files under /home/a/b/c

USAGE
  python etl_c86_pa_client360.py --ini-run N
  # or
  python etl_c86_pa_client360.py --ini-run Y

Requires a JSON file next to this script:
  TeradataConnection_T.json
  {
    "url": "td-hostname-or-ip",
    "user": "service_id",
    "password": "********"
  }

Dependencies:
  pip install pandas numpy teradatasql openpyxl
"""

import os
import sys
import json
import argparse
import logging
from logging.handlers import RotatingFileHandler
from datetime import date, datetime, timedelta
from typing import Dict, List, Optional

import numpy as np
import pandas as pd
import teradatasql  # DB-API


# ==============================================================================
# Constants / Paths
# ==============================================================================
OUT_DIR = "/home/a/b/c"
LOG_DIR = os.path.join(OUT_DIR, "logs")
AUTOCOMPLETE_XLSX = os.path.join(OUT_DIR, "pa_client360_autocomplete.xlsx")
DETAIL_XLSX_FMT = os.path.join(OUT_DIR, "{runday}", "pa_client360_detail_{runday}.xlsx")

CONN_JSON = "TeradataConnection_T.json"  # expected in the same folder as this script


# ==============================================================================
# Logging
# ==============================================================================
def setup_logging(verbosity: int = 0) -> None:
    os.makedirs(LOG_DIR, exist_ok=True)
    level = logging.DEBUG if verbosity > 0 else logging.INFO
    fmt = "%(asctime)s | %(levelname)s | %(message)s"

    handlers: List[logging.Handler] = [logging.StreamHandler(sys.stdout)]
    file_handler = RotatingFileHandler(
        filename=os.path.join(LOG_DIR, f"c86_pa_client360_{datetime.now():%Y%m%d}.log"),
        maxBytes=5_000_000,
        backupCount=5,
        encoding="utf-8",
    )
    handlers.append(file_handler)

    logging.basicConfig(level=level, format=fmt, handlers=handlers)
    logging.info("Logging initialized.")


# ==============================================================================
# Utilities
# ==============================================================================
def load_td_connection(json_filename: str) -> Dict[str, str]:
    script_dir = os.path.dirname(os.path.abspath(__file__))
    json_path = os.path.join(script_dir, json_filename)
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"Connection JSON not found: {json_path}")
    with open(json_path, "r", encoding="utf-8") as f:
        cfg = json.load(f)
    # minimal validation
    for k in ("url", "user", "password"):
        if not cfg.get(k):
            raise ValueError(f"Missing '{k}' in {json_path}")
    return cfg


class TDClient:
    """Simple Teradata client using teradatasql DB-API with a shared connection."""

    def __init__(self, host: str, user: str, password: str):
        self._conn = teradatasql.connect(host=host, user=user, password=password)
        logging.info("Connected to Teradata via teradatasql.")

    def read_sql(self, name: str, sql: str, params: Optional[dict] = None) -> pd.DataFrame:
        with self._conn.cursor() as cur:
            logging.debug(f"SQL[{name}]: {sql}")
            df = pd.read_sql(sql, self._conn, params=params or {})
        # Spec: uppercase cols + log
        df.columns = df.columns.str.upper()
        logging.info(f"{name}: {len(df):,} rows | Columns: {df.columns.tolist()}")
        return df

    def close(self):
        try:
            self._conn.close()
        except Exception:
            pass


def normalize_keys(df: pd.DataFrame, keys: List[str]) -> None:
    for k in keys:
        if k in df.columns:
            df[k] = (
                df[k]
                .astype(str)
                .str.strip()
                .str.upper()
                .replace({"NAN": np.nan})
            )
        else:
            raise KeyError(f"Missing join key: {k}")


def safe_merge(
    left: pd.DataFrame,
    right: pd.DataFrame,
    on: List[str],
    how: str,
    left_name: str,
    right_name: str,
    out_name: str,
) -> pd.DataFrame:
    normalize_keys(left, on)
    normalize_keys(right, on)
    n_l, n_r = len(left), len(right)
    out = left.merge(right, on=on, how=how, suffixes=("", "_R"))
    logging.info(
        f"{out_name}: merge({how}) {on} | {left_name}={n_l:,}, {right_name}={n_r:,} -> {len(out):,}"
    )
    return out


# ==============================================================================
# SAS Macro / Date logic replication (INI-RUN)
# ==============================================================================
def compute_dates(ini_run: str) -> Dict[str, str]:
    """
    Replicates the SAS data _null_ block:
      - launch_dt = '07MAY2023'd
      - launch_dt_min14 = '23APR2023'd
      - week_start = intnx('week.4', today(), 0, 'b') - 11;
      - week_end   = intnx('week.4', today(), 0, 'b') - 5;
      - runday = yymmddn8.
    Notes:
      * Exact SAS 'week.4' anchor is Friday-based; we approximate by taking the
        Monday start-of-week then subtracting 4 to align to Friday anchor,
        then apply -11/-5 day offsets.
      * For INI-RUN='Y': use launch dates for wk_start / wk_start_min14; wk_end = today().
      * For INI-RUN='N': use computed week_start and week_start-14.
    """
    today_dt = date.today()
    # Start of ISO week (Monday)
    monday = today_dt - timedelta(days=today_dt.weekday())
    # Approximate SAS intnx('week.4', today, 0, 'b'): Friday anchor -> Monday - 4
    anchor = monday - timedelta(days=4)
    week_start = anchor - timedelta(days=11)
    week_end = anchor - timedelta(days=5)

    launch_dt = date(2023, 5, 7)        # 07MAY2023
    launch_dt_min14 = date(2023, 4, 23) # 23APR2023

    if ini_run.upper() == "Y":
        wk_start = launch_dt
        wk_start_min14 = launch_dt_min14
        wk_end = today_dt
    else:
        wk_start = week_start
        wk_start_min14 = wk_start - timedelta(days=14)
        wk_end = today_dt

    runday = today_dt.strftime("%Y%m%d")  # yymmddn8.
    tday   = runday  # tracked just for parity

    out = {
        "WK_START": wk_start.strftime("%Y-%m-%d"),
        "WK_START_MIN14": wk_start_min14.strftime("%Y-%m-%d"),
        "WK_END": wk_end.strftime("%Y-%m-%d"),
        "RUNDAY": runday,
        "TDAY": tday,
        "INI_RUN": ini_run.upper(),
    }
    logging.info(f"DATE MACROS: {out}")
    return out


# ==============================================================================
# Main ETL
# ==============================================================================
def run_pipeline(ini_run: str = "N") -> int:
    os.makedirs(OUT_DIR, exist_ok=True)

    # 1) Dates / Macros
    macros = compute_dates(ini_run)

    # 2) Connect
    cfg = load_td_connection(CONN_JSON)
    td = TDClient(host=cfg["url"], user=cfg["user"], password=cfg["password"])

    # --------------------------------------------------------------------------
    # SECTION: Tracking (Advice Tool usage)
    # --------------------------------------------------------------------------
    SQL_TRACKING_ALL = f"""
        SELECT *
        FROM DDWV01.EVNT_PROD_TRACK_LOG
        WHERE advr_selt_typ = 'Advice Tool'
          AND EVNT_DT > DATE '{macros["WK_START"]}' - 90
    """
    tracking_all = td.read_sql("TRACKING_ALL", SQL_TRACKING_ALL)

    # Distinct tool use per OPPOR_ID (mirror PROC SQL)
    tracking_tool_use_distinct = (
        tracking_all.loc[
            tracking_all["OPPOR_ID"].notna() & tracking_all["ADVC_TOOL_NM"].notna(),
            ["OPPOR_ID", "ADVC_TOOL_NM"],
        ]
        .assign(ADVC_TOOL_NM=lambda d: d["ADVC_TOOL_NM"].astype(str).str.upper())
        .drop_duplicates()
        .reset_index(drop=True)
    )
    tracking_tool_use_distinct.columns = tracking_tool_use_distinct.columns.str.upper()
    logging.info(
        f"TRACKING_TOOL_USE_DISTINCT: {len(tracking_tool_use_distinct):,} rows | Columns: {tracking_tool_use_distinct.columns.tolist()}"
    )

    # Count unique tools per opportunity
    tracking_count_tool_use_pre2 = (
        tracking_all.loc[
            tracking_all["OPPOR_ID"].notna() & tracking_all["ADVC_TOOL_NM"].notna(),
            ["OPPOR_ID", "ADVC_TOOL_NM"],
        ]
        .assign(ADVC_TOOL_NM=lambda d: d["ADVC_TOOL_NM"].astype(str).str.upper())
        .drop_duplicates()
        .groupby("OPPOR_ID", as_index=False)
        .agg(count_unique_tool_used=("ADVC_TOOL_NM", "nunique"))
        .sort_values("count_unique_tool_used", ascending=False)
        .reset_index(drop=True)
    )
    tracking_count_tool_use_pre2.columns = tracking_count_tool_use_pre2.columns.str.upper()
    logging.info(
        f"TRACKING_COUNT_TOOL_USE_PRE2: {len(tracking_count_tool_use_pre2):,} rows | Columns: {tracking_count_tool_use_pre2.columns.tolist()}"
    )

    tracking_tool_use = tracking_count_tool_use_pre2.assign(
        TOOL_USED=lambda d: np.where(d["COUNT_UNIQUE_TOOL_USED"] > 0, "Tool Used", None)
    )[["OPPOR_ID", "TOOL_USED"]]
    tracking_tool_use.columns = tracking_tool_use.columns.str.upper()
    logging.info(
        f"TRACKING_TOOL_USE: {len(tracking_tool_use):,} rows | Columns: {tracking_tool_use.columns.tolist()}"
    )

    # --------------------------------------------------------------------------
    # SECTION: c360_short replacement (volatile) + DETAIL_PRE join
    #   We do NOT create a volatile table. We build equivalent dataframes
    #   and push only unaltered SQL for base pulls.
    # --------------------------------------------------------------------------
    SQL_C360_BASE = f"""
        SELECT
            EVNT_ID,
            RBC_OPPOR_OWN_ID,
            EVNT_DT,
            OPPOR_ID,
            PROD_REC_TYP AS OPPOR_REC_TYP,
            PROD_CD      AS PROD_CD,
            PROD_CATG_NM AS PROD_CATG_NM,
            ASCT_PROD_FMLY_NM AS ASCT_PROD_FMLY_NM,
            PROD_SRVC_NM AS PROD_SRVC_NM,
            OPPOR_STAGE_NM AS OPPOR_STAGE_NM
        FROM DDWV01.EVNT_PROD_OPPOR
        WHERE EVNT_ID IS NOT NULL
          AND EVNT_DT BETWEEN DATE '{macros["WK_START"]}' AND DATE '{macros["WK_END"]}'
          AND RBC_OPPOR_OWN_ID IS NOT NULL
    """
    c360_base = td.read_sql("C360_BASE", SQL_C360_BASE)

    # Build c360_short in pandas (volatile equivalent)
    c360_short = (
        c360_base.loc[:, ["EVNT_ID", "RBC_OPPOR_OWN_ID", "EVNT_DT"]]
        .rename(columns={"RBC_OPPOR_OWN_ID": "EMP_ID", "EVNT_DT": "SNAP_DT"})
        .assign(EMP_ID=lambda d: pd.to_numeric(d["EMP_ID"], errors="coerce").astype("Int64"))
        .dropna(subset=["EMP_ID", "SNAP_DT", "EVNT_ID"])
        .reset_index(drop=True)
    )
    c360_short.columns = c360_short.columns.str.upper()
    logging.info(f"C360_SHORT: {len(c360_short):,} rows | Columns: {c360_short.columns.tolist()}")

    # Employee dimension windows
    SQL_EMP = """
        SELECT EMP_ID, ORG_UNT_NO, HR_POSN_TITL_EN, CAPTR_DT, CHG_DT, OCCPT_JOB_CD
        FROM DDWV01.EMP
    """
    emp = td.read_sql("EMP", SQL_EMP)

    SQL_EMPL_RELTN = """
        SELECT EMP_ID, POSN_STRT_DT, POSN_END_DT, CAPTR_DT, CHG_DT
        FROM DDWV01.EMPL_RELTN
    """
    rel = td.read_sql("EMPL_RELTN", SQL_EMPL_RELTN)

    # Window joins per SAS:
    # inner join EMP on EMP_ID match and SNAP_DT within [CAPTR_DT, CHG_DT)
    c3_emp = safe_merge(
        c360_short, emp, on=["EMP_ID"], how="inner", left_name="C360_SHORT", right_name="EMP", out_name="C3xEMP"
    )
    c3_emp = c3_emp.loc[
        (pd.to_datetime(c3_emp["SNAP_DT"]) >= pd.to_datetime(c3_emp["CAPTR_DT"]))
        & (pd.to_datetime(c3_emp["SNAP_DT"]) < pd.to_datetime(c3_emp["CHG_DT"]))
    ][["EVNT_ID", "EMP_ID", "ORG_UNT_NO", "HR_POSN_TITL_EN", "OCCPT_JOB_CD", "SNAP_DT"]]

    # inner join REL on EMP_ID match and SNAP_DT within [CAPTR_DT, CHG_DT)
    c3_emp_rel = safe_merge(
        c3_emp, rel, on=["EMP_ID"], how="inner", left_name="C3xEMP", right_name="EMPL_RELTN", out_name="C3xEMPxREL"
    )
    c3_emp_rel = c3_emp_rel.loc[
        (pd.to_datetime(c3_emp_rel["SNAP_DT"]) >= pd.to_datetime(c3_emp_rel["CAPTR_DT"]))
        & (pd.to_datetime(c3_emp_rel["SNAP_DT"]) < pd.to_datetime(c3_emp_rel["CHG_DT"]))
    ][["EVNT_ID", "ORG_UNT_NO", "HR_POSN_TITL_EN", "POSN_STRT_DT", "POSN_END_DT", "OCCPT_JOB_CD"]].drop_duplicates()

    # LEFT JOIN onto full c360 set within week window
    c360_detail_pre = safe_merge(
        c360_base,
        c3_emp_rel,
        on=["EVNT_ID"],
        how="left",
        left_name="C360_BASE",
        right_name="C3_EMP_REL",
        out_name="C360_DETAIL_PRE",
    )

    # Add tool flag
    c360_detail = safe_merge(
        c360_detail_pre,
        tracking_tool_use,
        on=["OPPOR_ID"],
        how="left",
        left_name="C360_DETAIL_PRE",
        right_name="TRACKING_TOOL_USE",
        out_name="C360_DETAIL",
    )
    # TOOL_USED missing => 'Tool Not Used'
    c360_detail["TOOL_USED"] = c360_detail["TOOL_USED"].fillna("Tool Not Used")

    # --------------------------------------------------------------------------
    # SECTION: AOT link (past 14 days up to wk_end)
    # --------------------------------------------------------------------------
    SQL_AOT_ALL = f"""
        SELECT OPPOR_ID, COUNT(*) AS COUNT_AOT
        FROM DDWV01.EVNT_PROD_AOT
        WHERE ESS_SRC_EVNT_DT BETWEEN DATE '{macros["WK_START_MIN14"]}' AND DATE '{macros["WK_END"]}'
          AND OPPOR_ID IS NOT NULL
        GROUP BY 1
    """
    aot_all_oppor = td.read_sql("AOT_ALL_OPPOR", SQL_AOT_ALL)

    aot_all_oppor_unique = aot_all_oppor[["OPPOR_ID"]].drop_duplicates().reset_index(drop=True)
    aot_all_oppor_unique.columns = aot_all_oppor_unique.columns.str.upper()
    logging.info(
        f"AOT_ALL_OPPOR_UNIQUE: {len(aot_all_oppor_unique):,} rows | Columns: {aot_all_oppor_unique.columns.tolist()}"
    )

    c360_detail_link_aot = safe_merge(
        c360_detail, aot_all_oppor_unique, on=["OPPOR_ID"], how="left",
        left_name="C360_DETAIL", right_name="AOT_OPPOR_UNQ", out_name="C360_DETAIL_LINK_AOT",
    ).rename(columns={"OPPOR_ID_R": "AOT_OPPOR_ID"})
    c360_detail_link_aot["C360_PDA_LINK_AOT"] = np.where(
        (c360_detail_link_aot["PROD_CATG_NM"] == "Personal Accounts")
        & (c360_detail_link_aot["OPPOR_ID"].notna()),
        1, 0
    )

    # --------------------------------------------------------------------------
    # SECTION: Stage format mapping and filter to C360_DETAIL_MORE
    # --------------------------------------------------------------------------
    stagefmt = {
        "DÃ‰marche exploratoire/Comprendre le besoin": "12.Discovery/Understand Needs",
        "Discovery/Understand Needs": "12.Discovery/Understand Needs",
        "Review Options": "21.Review Options",
        "Present/Gain Commitment": "31.Present/Gain Commitment",
        "IntÃ©gration commencÃ©e": "41.IntÃ¨gration commencÃ©e",
        "Onboarding Started": "42.Onboarding Started",
        "Opportunity Lost": "51.Opportunity Lost",
        "Opportunity Won": "61.Opportunity Won",
    }

    df_more = c360_detail_link_aot.copy()
    df_more["OPPOR_STAGE_NM_F"] = df_more["OPPOR_STAGE_NM"].map(stagefmt).fillna(df_more["OPPOR_STAGE_NM"])

    df_more_filter = df_more.loc[
        (df_more["ASCT_PROD_FMLY_NM"] != "Risk Protection")
        & (df_more["LOB"] == "Retail")
        & (df_more["C360_PDA_LINK_AOT"] == 0)
        & (df_more["OPPOR_STAGE_NM"].isin(["Opportunity Won", "Opportunity Lost"]))
    ].copy()

    # --------------------------------------------------------------------------
    # SECTION: PA rationale (text validity) â€” work.pa_rationale
    #   Valid if: >5 chars, not repeated chars only, >=2 distinct chars, >=2 alnum
    # --------------------------------------------------------------------------
    c360_detail_more_io = df_more_filter.copy()
    pa_rationale_src = c360_detail_more_io.loc[
        c360_detail_more_io["IS_PROD_ARPR_FOR_CLNT"] == "Not Appropriate - Rationale",
        ["EVNT_ID", "IS_PROD_ARPR_FOR_CLNT", "CLNT_RTNL_TXT"],
    ].copy()

    if not pa_rationale_src.empty:
        _x = pa_rationale_src["CLNT_RTNL_TXT"].astype(str)
        # normalize spaces + case-insensitive comparisons
        _xp = _x.str.replace(r"\s+", " ", regex=True)
        _x1 = _xp.str.upper().str.strip()

        xfail_chars_gt5 = (_x1.str.len() > 5).astype(int)
        # "only repeated characters" failure -> detect 1 unique char after strip
        x2_first = _x1.str[:1]
        x2_rest  = _x1.str[1:]
        xfail_rep_char = (~((x2_rest != x2_first).any() if isinstance(x2_rest, pd.Series) else True)).astype(int)
        # At least two different characters:
        xfail_diff = (_x1.apply(lambda s: len(set(s)) >= 2)).astype(int)
        # At least two alnum:
        xfail_ge2_alnum = (_x1.str.replace(r"[^A-Z0-9]", "", regex=True).str.len() >= 2).astype(int)

        # valid if all checks pass
        valid = (xfail_chars_gt5 & xfail_diff & xfail_ge2_alnum).astype(int)
        pa_rationale = pa_rationale_src.copy()
        pa_rationale["PROD_NOT_APPR_RTNL_TXT_CAT"] = np.where(valid == 1, "Valid", "Invalid")
        pa_rationale.columns = pa_rationale.columns.str.upper()
    else:
        pa_rationale = pd.DataFrame(columns=["EVNT_ID", "IS_PROD_ARPR_FOR_CLNT", "CLNT_RTNL_TXT", "PROD_NOT_APPR_RTNL_TXT_CAT"])

    logging.info(f"PA_RATIONALE: {len(pa_rationale):,} rows | Columns: {pa_rationale.columns.tolist()}")

    # Join rationale back -> C360_DETAIL_MORE_IN
    c360_detail_more_in = safe_merge(
        c360_detail_more_io, pa_rationale[["EVNT_ID", "PROD_NOT_APPR_RTNL_TXT_CAT"]],
        on=["EVNT_ID"], how="left",
        left_name="C360_DETAIL_MORE_IO", right_name="PA_RATIONALE", out_name="C360_DETAIL_MORE_IN"
    )

    # For missing / non-'Not Appropriate - Rationale', derive category per SAS CASE
    def _rtnl_cat(row):
        v = row.get("IS_PROD_ARPR_FOR_CLNT")
        if pd.isna(v):
            return "Not Available"
        if v != "Not Appropriate - Rationale":
            return "Not Applicable"
        return row.get("PROD_NOT_APPR_RTNL_TXT_CAT")

    c360_detail_more_in["PROD_NOT_APPR_RTNL_TXT_CAT"] = c360_detail_more_in.apply(_rtnl_cat, axis=1)

    # --------------------------------------------------------------------------
    # SECTION: De-dup by OPPOR_ID (BY group level_oppor = first)
    # --------------------------------------------------------------------------
    tmp = c360_detail_more_in.sort_values(["OPPOR_ID", "EVNT_ID"]).copy()
    tmp["LEVEL_OPPOR"] = (tmp.groupby("OPPOR_ID").cumcount() + 1)
    tmp_first = tmp.loc[tmp["LEVEL_OPPOR"] == 1].copy()

    # --------------------------------------------------------------------------
    # SECTION: Build tmp_pa_C360_4ac (detail rows)
    # --------------------------------------------------------------------------
    def snap_week_end(d):
        # intnx('week.7', evnt_dt, 0, 'e') -> week ending on Sunday; compute upcoming Sunday
        if pd.isna(d):
            return pd.NaT
        d = pd.to_datetime(d)
        return d + timedelta(days=(6 - d.weekday()))

    tmp_pa_C360_4ac = tmp_first.copy()
    tmp_pa_C360_4ac["REGULATORYNAME"] = "C86"
    tmp_pa_C360_4ac["LOB"] = "Retail"
    tmp_pa_C360_4ac["REPORTNAME"] = "C86 Client360 Product Appropriateness"
    tmp_pa_C360_4ac["CONTROLRISK"] = "Completeness"
    tmp_pa_C360_4ac["TESTTYPE"] = "Anomaly"
    tmp_pa_C360_4ac["TESTPERIOD"] = "Origination"
    tmp_pa_C360_4ac["PRODUCTTYPE"] = tmp_pa_C360_4ac["PROD_CATG_NM"]
    tmp_pa_C360_4ac["SEGMENT"] = "Account Open"
    tmp_pa_C360_4ac["SEGMENT2"] = tmp_pa_C360_4ac["ASCT_PROD_FMLY_NM"]
    tmp_pa_C360_4ac["SEGMENT3"] = tmp_pa_C360_4ac["PROD_SRVC_NM"]
    tmp_pa_C360_4ac["SEGMENT6"] = tmp_pa_C360_4ac["OPPOR_STAGE_NM"]
    tmp_pa_C360_4ac["SEGMENT7"] = tmp_pa_C360_4ac["TOOL_USED"]
    tmp_pa_C360_4ac["SEGMENT10"] = pd.to_datetime(tmp_pa_C360_4ac["EVNT_DT"]).dt.strftime("%Y%m")
    tmp_pa_C360_4ac["COMMENTCODE"] = "COM13"
    tmp_pa_C360_4ac["COMMENTS"] = "Population Distribution"
    tmp_pa_C360_4ac["HOLDFLAG"] = "N"
    tmp_pa_C360_4ac["SNAPDATE"] = pd.to_datetime(tmp_pa_C360_4ac["EVNT_DT"]).map(snap_week_end)
    tmp_pa_C360_4ac["DATECOMPLETED"] = pd.to_datetime(date.today())
    # Keep SAS-like format by logging; Excel will render dates.

    logging.info(f"TMP_PA_C360_4AC: {len(tmp_pa_C360_4ac):,} rows")

    # --------------------------------------------------------------------------
    # SECTION: Tool-use enrichment for counts
    # --------------------------------------------------------------------------
    tmp_pa_C360_4ac_count_pre = safe_merge(
        tmp_pa_C360_4ac,
        tracking_tool_use_distinct.rename(columns={"ADVC_TOOL_NM": "ADVC_TOOL_NM"}),
        on=["OPPOR_ID"], how="left",
        left_name="TMP_PA_C360_4AC",
        right_name="TRACKING_TOOL_USE_DISTINCT",
        out_name="TMP_PA_C360_4AC_COUNT_PRE",
    )

    tmp_pa_C360_4ac_count = tmp_pa_C360_4ac_count_pre.copy()
    tmp_pa_C360_4ac_count["SEGMENT8"] = tmp_pa_C360_4ac_count["ADVC_TOOL_NM"]
    tmp_pa_C360_4ac_count["SEGMENT10"] = pd.to_datetime(tmp_pa_C360_4ac_count["EVNT_DT"]).dt.strftime("%Y%m")
    tmp_pa_C360_4ac_count["SNAPDATE"] = pd.to_datetime(tmp_pa_C360_4ac_count["EVNT_DT"]).map(snap_week_end)
    tmp_pa_C360_4ac_count["DATECOMPLETED"] = pd.to_datetime(date.today())

    logging.info(f"TMP_PA_C360_4AC_COUNT: {len(tmp_pa_C360_4ac_count):,} rows")

    # --------------------------------------------------------------------------
    # SECTION: Assessments (AC & Count assessments)
    # --------------------------------------------------------------------------
    def _segment4(val: str) -> str:
        if val == "Product Appropriateness assessed outside Client 360":
            return "Product Appropriateness assessed outside of Client360"
        if val == "Not Appropriate - Rationale":
            return "Product Not Appropriate"
        if val == "Client declined product appropriateness assessment":
            return "Client declined product appropriateness assessment"
        if val == "Product Appropriate":
            return "Product Appropriate"
        return "Missing"

    # AC assessment
    pa_c360_ac_assessment = tmp_pa_C360_4ac.copy()
    pa_c360_ac_assessment["RDE"] = "PA002_Client360_Completeness_RDE"
    pa_c360_ac_assessment["SEGMENT4"] = pa_c360_ac_assessment["IS_PROD_ARPR_FOR_CLNT"].apply(_segment4)
    pa_c360_ac_assessment["SEGMENT5"] = pa_c360_ac_assessment["PROD_NOT_APPR_RTNL_TXT_CAT"]
    grp_cols = [
        "REGULATORYNAME","LOB","REPORTNAME","CONTROLRISK","TESTTYPE","TESTPERIOD","PRODUCTTYPE",
        "RDE","SEGMENT","SEGMENT2","SEGMENT3","SEGMENT4","SEGMENT5","SEGMENT6","SEGMENT7","SEGMENT10",
        "HOLDFLAG","COMMENTCODE","COMMENTS","DATECOMPLETED","SNAPDATE"
    ]
    pa_c360_ac_assessment = (
        pa_c360_ac_assessment
        .groupby(grp_cols, dropna=False, as_index=False)
        .agg(Volume=("EVNT_ID","count"), Amount=("EVNT_ID","count"))
    )
    logging.info(f"PA_C360_AC_ASSESSMENT: {len(pa_c360_ac_assessment):,} rows")

    # Count assessment
    pa_c360_ac_count_assessment = tmp_pa_C360_4ac_count.copy()
    pa_c360_ac_count_assessment["RDE"] = "PA003_Client360_Completeness_Tool"
    pa_c360_ac_count_assessment["SEGMENT4"] = pa_c360_ac_count_assessment["IS_PROD_ARPR_FOR_CLNT"].apply(_segment4)
    pa_c360_ac_count_assessment["SEGMENT5"] = pa_c360_ac_count_assessment["PROD_NOT_APPR_RTNL_TXT_CAT"]
    grp_cols_cnt = grp_cols[:-2] + ["SEGMENT8","DATECOMPLETED","SNAPDATE"]
    pa_c360_ac_count_assessment = (
        pa_c360_ac_count_assessment
        .groupby(grp_cols_cnt, dropna=False, as_index=False)
        .agg(Volume=("EVNT_ID","count"), Amount=("EVNT_ID","count"))
    )
    logging.info(f"PA_C360_AC_COUNT_ASSESSMENT: {len(pa_c360_ac_count_assessment):,} rows")

    # --------------------------------------------------------------------------
    # SECTION: Autocomplete build + append
    #   combine_pa_autocomplete = union of count + tool_use tabs
    # --------------------------------------------------------------------------
    # As in SAS: combine both sets (matching templat structure)
    pa_C360_autocomplete_Count_Tool = pa_c360_ac_count_assessment.copy()
    pa_C360_autocomplete_tool_use = pa_c360_ac_assessment.copy()

    combine_pa_autocomplete = pd.concat(
        [pa_C360_autocomplete_Count_Tool, pa_C360_autocomplete_tool_use],
        ignore_index=True,
        sort=False,
    )

    # Append to base Excel file (create or dedupe current runday first)
    runday = macros["RUNDAY"]
    if os.path.exists(AUTOCOMPLETE_XLSX):
        base = pd.read_excel(AUTOCOMPLETE_XLSX, sheet_name="autocomplete")
        base.columns = base.columns.str.upper()
        # Drop any existing rows with DateCompleted == runday
        if "DATECOMPLETED" in base.columns:
            base = base.loc[pd.to_datetime(base["DATECOMPLETED"]).dt.strftime("%Y%m%d") != runday]
        combined = pd.concat([base, combine_pa_autocomplete], ignore_index=True, sort=False)
    else:
        combined = combine_pa_autocomplete.copy()

    with pd.ExcelWriter(AUTOCOMPLETE_XLSX, engine="openpyxl") as xw:
        combined.to_excel(xw, sheet_name="autocomplete", index=False)
    logging.info(f"Autocomplete Excel updated: {AUTOCOMPLETE_XLSX}")

    # --------------------------------------------------------------------------
    # SECTION: Detail export (subset on PA_result categories)
    # Mirrors PROC SQL select from tmp_pa_C360_4ac_count_pre
    # --------------------------------------------------------------------------
    tmp_detail = tmp_pa_C360_4ac_count_pre.copy()
    tmp_detail["PA_RESULT"] = tmp_detail["IS_PROD_ARPR_FOR_CLNT"].apply(_segment4)

    keep_values = {
        "Product Not Appropriate",
        "Missing",
        "Product Appropriateness assessed outside of Client360",
    }
    detail_final = tmp_detail.loc[tmp_detail["PA_RESULT"].isin(keep_values)].copy()

    out_detail_path = DETAIL_XLSX_FMT.format(runday=runday)
    os.makedirs(os.path.dirname(out_detail_path), exist_ok=True)
    with pd.ExcelWriter(out_detail_path, engine="openpyxl") as xw:
        detail_final.to_excel(xw, sheet_name="detail", index=False)
    logging.info(f"Detail Excel written: {out_detail_path}")

    # Close TD
    td.close()

    logging.info("ETL finished successfully.")
    return 0


# ==============================================================================
# CLI
# ==============================================================================
def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="C86 Client360 PA â€” SASâ†’Python exact replica")
    p.add_argument("--ini-run", default="N", choices=["N", "Y"], help="INI-RUN flag (default N)")
    p.add_argument("-v", "--verbose", action="count", default=0, help="Increase log verbosity")
    return p.parse_args()


def main():
    args = parse_args()
    setup_logging(args.verbose)
    rc = run_pipeline(ini_run=args.ini_run)
    sys.exit(rc)


if __name__ == "__main__":
    main()
